{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle & Analyze Data on WeRateDogs tweets!\n",
    "\n",
    "\n",
    "In this project from the Udacity Data Analyst nanodegree I have shown how to efficiently do data wrangling and analyze real-world messy data!\n",
    "The data I wrangled is the tweet archive of [@dog_rates](https://twitter.com/dog_rates), which is a Twitter account that rates people's dogs with funny comments and an [unusual rating system](https://knowyourmeme.com/memes/theyre-good-dogs-brent). \n",
    "\n",
    "### Data\n",
    "Starting from the data, I immediately realized how messy it was and how much data was missing to create any interesting analysis. The dataset contains partial tweet data for 2356 tweets, one important column the archive does contain is each tweet's text, which Udacity used to extract rating, dog name, and dog stage, but it didn't do a good job.\n",
    "\n",
    "### Data Gathering\n",
    "After a quick look at the archive I immediately started by gathering some additional data. I started by downloading another file provided by Udacity, containing the URLs of dog images related to the same tweets as well as some predictions about the dog's breed estimated with a neural network. The second part of data gathering was done by restoring the archive with some of the many missing information. I then decided to use the Twitter API with python to retrieve all the tweets in the archive, at this point I was ready to inspect the data.\n",
    "\n",
    "### Data Assessment\n",
    "Once I gathered all the data it was time to check it. I found a lot of issues, both quality and tidiness issues, like i.e. missing data, wrong data extracted from the text, duplicates, tables not well formatted etc.. I could do almost everything with programmatic assessment with python and pandas.\n",
    "\n",
    "### Data Cleaning\n",
    "After spotting many issues, it was time to fix them. Also in this case I used python and pandas for a quick and efficient cleaning process. This process, together with data assessment had be to repeated for a couple of times to make sure I was not missing anything on the way.\n",
    "\n",
    "### Analysis and Visualizations\n",
    "After all this wrangling I could finally analyze the data and here are some insights I found.\n",
    "\n",
    "![tweets](images/Top_10_tweets.png)\n",
    "![likes](images/Top_10_likes.png)\n",
    "![retweets](images/Top_10_retweets.png)\n",
    "\n",
    "I also found the highest rated (with this strange rating system :)) breed, which is Clumber! Usually the ratings are in the range 9/10 to 12/10 (yes, 12 this is not a typo :)), but this puppy scored 27/10, here is the picture!\n",
    "\n",
    "![dog](images/highest_rated.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
